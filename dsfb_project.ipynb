{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science For Business Project\n",
    "\n",
    "# Step 1\n",
    "\n",
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>category</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B000J4HXUC</td>\n",
       "      <td>Sports_and_Outdoors</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>5</td>\n",
       "      <td>It's a .50 Caliber Ammo Can. That largely sums...</td>\n",
       "      <td>01 5, 2014</td>\n",
       "      <td>A3QRW0UJPKIAX7</td>\n",
       "      <td>Grant Fritchey</td>\n",
       "      <td>Clean and Exactly as Advertised</td>\n",
       "      <td>1388880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0983393214</td>\n",
       "      <td>Books</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>This was a very good book. It kept me excited ...</td>\n",
       "      <td>06 23, 2013</td>\n",
       "      <td>A2SEIOM4H06WTH</td>\n",
       "      <td>TJ</td>\n",
       "      <td>Great read!</td>\n",
       "      <td>1371945600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B003G4FVMY</td>\n",
       "      <td>Grocery_and_Gourmet_Food</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>If you love coconut the way I do, you can't go...</td>\n",
       "      <td>05 19, 2013</td>\n",
       "      <td>A3GDEXMU9587JX</td>\n",
       "      <td>K. Parsley \"kindlekat\"</td>\n",
       "      <td>If you love coconut, get this coffee</td>\n",
       "      <td>1368921600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00F9VRNF0</td>\n",
       "      <td>Cell_Phones_and_Accessories</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>I recently switched from the Galaxy S3 to the ...</td>\n",
       "      <td>04 25, 2014</td>\n",
       "      <td>ASP3J2NEHDN4E</td>\n",
       "      <td>ChriS</td>\n",
       "      <td>Superior Protection!!!</td>\n",
       "      <td>1398384000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00D5OZQUC</td>\n",
       "      <td>Amazon_Instant_Video</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>Good show,looks like the gap from season 2 to ...</td>\n",
       "      <td>11 4, 2013</td>\n",
       "      <td>A1EDBI6TBKP9CO</td>\n",
       "      <td>Grants Book Trade</td>\n",
       "      <td>Love the show, thanks for putting Season 3 on ...</td>\n",
       "      <td>1383523200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin                     category helpful  overall  \\\n",
       "0  B000J4HXUC          Sports_and_Outdoors  [1, 1]        5   \n",
       "1  0983393214                        Books  [0, 0]        5   \n",
       "2  B003G4FVMY     Grocery_and_Gourmet_Food  [0, 0]        5   \n",
       "3  B00F9VRNF0  Cell_Phones_and_Accessories  [0, 0]        5   \n",
       "4  B00D5OZQUC         Amazon_Instant_Video  [0, 0]        5   \n",
       "\n",
       "                                          reviewText   reviewTime  \\\n",
       "0  It's a .50 Caliber Ammo Can. That largely sums...   01 5, 2014   \n",
       "1  This was a very good book. It kept me excited ...  06 23, 2013   \n",
       "2  If you love coconut the way I do, you can't go...  05 19, 2013   \n",
       "3  I recently switched from the Galaxy S3 to the ...  04 25, 2014   \n",
       "4  Good show,looks like the gap from season 2 to ...   11 4, 2013   \n",
       "\n",
       "       reviewerID            reviewerName  \\\n",
       "0  A3QRW0UJPKIAX7          Grant Fritchey   \n",
       "1  A2SEIOM4H06WTH                      TJ   \n",
       "2  A3GDEXMU9587JX  K. Parsley \"kindlekat\"   \n",
       "3   ASP3J2NEHDN4E                   ChriS   \n",
       "4  A1EDBI6TBKP9CO       Grants Book Trade   \n",
       "\n",
       "                                             summary  unixReviewTime  \n",
       "0                    Clean and Exactly as Advertised      1388880000  \n",
       "1                                        Great read!      1371945600  \n",
       "2               If you love coconut, get this coffee      1368921600  \n",
       "3                             Superior Protection!!!      1398384000  \n",
       "4  Love the show, thanks for putting Season 3 on ...      1383523200  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load file (we keep the git repo as light as possible by only hosting the .gz's)\n",
    "!rm -f *.json\n",
    "!gunzip -c amazon_step1.json.gz > amazon_step1.json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# A first look at the data\n",
    "df1 = pd.read_json('amazon_step1.json', lines=True)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "asin              object\n",
       "category          object\n",
       "helpful           object\n",
       "overall            int64\n",
       "reviewText        object\n",
       "reviewTime        object\n",
       "reviewerID        object\n",
       "reviewerName      object\n",
       "summary           object\n",
       "unixReviewTime     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 10\n",
      "Number of samples: 96000\n"
     ]
    }
   ],
   "source": [
    "num_total_samples = len(df1.index)\n",
    "num_total_features = len(df1.columns)\n",
    "print(\"Number of features:\", num_total_features)\n",
    "print(\"Number of samples:\", num_total_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of damaged samples: 994\n",
      "Percentage of damaged Samples: 1.0 %\n"
     ]
    }
   ],
   "source": [
    "num_valid_entries_per_sample = df1.count(axis=1)\n",
    "\n",
    "num_complete_samples = num_valid_entries_per_sample.tolist().count(num_total_features)\n",
    "\n",
    "percentage_damaged_samples = 1 - num_complete_samples/num_total_samples\n",
    "print('Number of damaged samples:', num_total_samples - num_complete_samples)\n",
    "print('Percentage of damaged Samples:', np.around(100*percentage_damaged_samples,decimals=1), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that we have some missing data. <br />\n",
    "Let's see the number of valid entries for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviewerName      95006\n",
      "asin              96000\n",
      "category          96000\n",
      "helpful           96000\n",
      "overall           96000\n",
      "reviewText        96000\n",
      "reviewTime        96000\n",
      "reviewerID        96000\n",
      "summary           96000\n",
      "unixReviewTime    96000\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "num_valid_entries_per_feature = df1.count(axis=0).sort_values()\n",
    "print(num_valid_entries_per_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only `reviewerName` is sometimes missing.<br />\n",
    "## We now present the preprocessing choices for each features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `reviewText` to predict the `category`, we drop the other features:<br />\n",
    "TODO: use summary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "df1 = df1.drop([\"asin\", \"helpful\", \"overall\", \"reviewTime\", \"reviewerID\", \"reviewerName\", \"summary\", \"unixReviewTime\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Category` is the target variable.<br />\n",
    "We use OneHotEncoding since it is needed for feeding categorical data to many sklearn estimators, notably SVMs , which we are required to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "one_hot = pd.get_dummies(df1[\"category\"])\n",
    "#df1 = df1.drop(\"category\", 1)\n",
    "df1 = df1.join(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reviewText\n",
    "We add the feature $length(reviewText)$ because we think it may be correlated to the target.<br />\n",
    "We standardize it: that way, outliers (e.g. very long comments) won't affect too much our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/com402/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "def text_length(row):\n",
    "    return len(row)\n",
    "df1[\"reviewTextLength\"] = df1[\"reviewText\"].apply(text_length)\n",
    "df1[\"reviewTextLength\"] = preprocessing.scale(pd.to_numeric(df1[\"reviewTextLength\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we also create features that capture the amount of punctuation that is used (standardized):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/com402/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "def count_char(row, char):\n",
    "    return row.count(char)\n",
    "df1[\"rtCountPoints\"] = df1[\"reviewText\"].apply(count_char, args=(\".\"))\n",
    "df1[\"rtCountPoints\"] = preprocessing.scale(df1[\"rtCountPoints\"])\n",
    "df1[\"rtCountExcl\"] = df1[\"reviewText\"].apply(count_char, args=(\"!\"))\n",
    "df1[\"rtCountExcl\"] = preprocessing.scale(df1[\"rtCountExcl\"])\n",
    "df1[\"rtCountInterr\"] = df1[\"reviewText\"].apply(count_char, args=(\"?\"))\n",
    "df1[\"rtCountInterr\"] = preprocessing.scale(df1[\"rtCountInterr\"])\n",
    "df1[\"rtCountComas\"] = df1[\"reviewText\"].apply(count_char, args=(\",\"))\n",
    "df1[\"rtCountComas\"] = preprocessing.scale(df1[\"rtCountComas\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing results\n",
    "We give below an overview of the final data frame that we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>Amazon_Instant_Video</th>\n",
       "      <th>Apps_for_Android</th>\n",
       "      <th>Automotive</th>\n",
       "      <th>Baby</th>\n",
       "      <th>Beauty</th>\n",
       "      <th>Books</th>\n",
       "      <th>CDs_and_Vinyl</th>\n",
       "      <th>Cell_Phones_and_Accessories</th>\n",
       "      <th>...</th>\n",
       "      <th>Pet_Supplies</th>\n",
       "      <th>Sports_and_Outdoors</th>\n",
       "      <th>Tools_and_Home_Improvement</th>\n",
       "      <th>Toys_and_Games</th>\n",
       "      <th>Video_Games</th>\n",
       "      <th>reviewTextLength</th>\n",
       "      <th>rtCountPoints</th>\n",
       "      <th>rtCountExcl</th>\n",
       "      <th>rtCountInterr</th>\n",
       "      <th>rtCountComas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sports_and_Outdoors</td>\n",
       "      <td>It's a .50 Caliber Ammo Can. That largely sums...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.106836</td>\n",
       "      <td>0.396455</td>\n",
       "      <td>-0.341411</td>\n",
       "      <td>-0.218245</td>\n",
       "      <td>-0.187700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Books</td>\n",
       "      <td>This was a very good book. It kept me excited ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.627738</td>\n",
       "      <td>-0.470575</td>\n",
       "      <td>-0.341411</td>\n",
       "      <td>-0.218245</td>\n",
       "      <td>-0.567760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grocery_and_Gourmet_Food</td>\n",
       "      <td>If you love coconut the way I do, you can't go...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.547035</td>\n",
       "      <td>-0.687332</td>\n",
       "      <td>0.215455</td>\n",
       "      <td>-0.218245</td>\n",
       "      <td>-0.441073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cell_Phones_and_Accessories</td>\n",
       "      <td>I recently switched from the Galaxy S3 to the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.256557</td>\n",
       "      <td>1.155105</td>\n",
       "      <td>1.886052</td>\n",
       "      <td>-0.218245</td>\n",
       "      <td>-0.314387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amazon_Instant_Video</td>\n",
       "      <td>Good show,looks like the gap from season 2 to ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.573936</td>\n",
       "      <td>-0.578953</td>\n",
       "      <td>-0.341411</td>\n",
       "      <td>2.760569</td>\n",
       "      <td>-0.187700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      category  \\\n",
       "0          Sports_and_Outdoors   \n",
       "1                        Books   \n",
       "2     Grocery_and_Gourmet_Food   \n",
       "3  Cell_Phones_and_Accessories   \n",
       "4         Amazon_Instant_Video   \n",
       "\n",
       "                                          reviewText  Amazon_Instant_Video  \\\n",
       "0  It's a .50 Caliber Ammo Can. That largely sums...                     0   \n",
       "1  This was a very good book. It kept me excited ...                     0   \n",
       "2  If you love coconut the way I do, you can't go...                     0   \n",
       "3  I recently switched from the Galaxy S3 to the ...                     0   \n",
       "4  Good show,looks like the gap from season 2 to ...                     1   \n",
       "\n",
       "   Apps_for_Android  Automotive  Baby  Beauty  Books  CDs_and_Vinyl  \\\n",
       "0                 0           0     0       0      0              0   \n",
       "1                 0           0     0       0      1              0   \n",
       "2                 0           0     0       0      0              0   \n",
       "3                 0           0     0       0      0              0   \n",
       "4                 0           0     0       0      0              0   \n",
       "\n",
       "   Cell_Phones_and_Accessories      ...       Pet_Supplies  \\\n",
       "0                            0      ...                  0   \n",
       "1                            0      ...                  0   \n",
       "2                            0      ...                  0   \n",
       "3                            1      ...                  0   \n",
       "4                            0      ...                  0   \n",
       "\n",
       "   Sports_and_Outdoors  Tools_and_Home_Improvement  Toys_and_Games  \\\n",
       "0                    1                           0               0   \n",
       "1                    0                           0               0   \n",
       "2                    0                           0               0   \n",
       "3                    0                           0               0   \n",
       "4                    0                           0               0   \n",
       "\n",
       "   Video_Games  reviewTextLength  rtCountPoints  rtCountExcl  rtCountInterr  \\\n",
       "0            0         -0.106836       0.396455    -0.341411      -0.218245   \n",
       "1            0         -0.627738      -0.470575    -0.341411      -0.218245   \n",
       "2            0         -0.547035      -0.687332     0.215455      -0.218245   \n",
       "3            0          1.256557       1.155105     1.886052      -0.218245   \n",
       "4            0         -0.573936      -0.578953    -0.341411       2.760569   \n",
       "\n",
       "   rtCountComas  \n",
       "0     -0.187700  \n",
       "1     -0.567760  \n",
       "2     -0.441073  \n",
       "3     -0.314387  \n",
       "4     -0.187700  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, tuning and testing of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5334fe87585e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# TODO: idk how to make this work\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtext_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mtext_clf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/com402/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \"\"\"\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/com402/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/com402/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 839\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/com402/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/com402/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 241\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/com402/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Set up train and test sets\n",
    "seed = 2017\n",
    "features = df1[[\"reviewText\"]]\n",
    "target = df1[[\"category\"]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=seed)\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=seed)),\n",
    "                    ])\n",
    "\n",
    "\n",
    "# TODO: idk how to make this work\n",
    "text_clf = text_clf.fit(X_train.values.reshape(len(X_train), 1), y_train)\n",
    "text_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2\n",
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!gunzip -c amazon_step23.json.gz > amazon_step23.json\n",
    "# TODO\n",
    "#df2 = pd.read_json('amazon_step23.json.gz', lines=True)\n",
    "#df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Step 3\n",
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
